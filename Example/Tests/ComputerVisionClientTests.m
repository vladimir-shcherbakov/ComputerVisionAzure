/**
 * Code generated by Microsoft (R) AutoRest Code Generator.
 * Changes may cause incorrect behavior and will be lost if the code is
 * regenerated.
 */

#import <XCTest/XCTest.h>
#import <ComputerVisionAzure/ComputerVisionAzure-umbrella.h>

@interface ComputerVisionClientTests : XCTestCase
    @property id<ComputerVisionClientProtocol> service;
@end
@implementation ComputerVisionClientTests
- (void)setUp {
    // Put setup code here. This method is called before the invocation of each test method in the class.
    self.continueAfterFailure = NO;
    self.service = [ComputerVisionClient create];
}
- (void)tearDown {
    // Put teardown code here. This method is called after the invocation of each test method in the class.
}

/**
 * This operation returns the list of domain-specific models that are supported by the Computer Vision API.  Currently, the API only supports one domain-specific model: a celebrity recognizer. A successful response will be returned in JSON.  If the request failed, the response will contain an error code and a message to help understand what went wrong.
 *
 */
- (void) test__listModels {
    XCTestExpectation *waitingLoading = [self expectationWithDescription:@"Wait for HTTP request to complete"];
    [self.service listModelsWithCallback:^(ListModelsResult* result, OperationError* error) {
        [waitingLoading fulfill];
        //XCTAssertNil(error, @"%@", error.reason);
    }];
    [self waitForExpectationsWithTimeout:100 handler:^(NSError *error) {
        if (error) {XCTFail(@"After block was not called.");}
    }];
}

/**
 * This operation extracts a rich set of visual features based on the image content. Two input methods are supported -- (1) Uploading an image or (2) specifying an image URL.  Within your request, there is an optional parameter to allow you to choose which features to return.  By default, image categories are returned in the response.
 *
 * body parameter: url Publicly reachable URL of an image
 * body parameter: visualFeatures A string indicating what visual feature types to return. Multiple values should be comma-separated. Valid visual feature types include:Categories - categorizes image content according to a taxonomy defined in documentation. Tags - tags the image with a detailed list of words related to the image content. Description - describes the image content with a complete English sentence. Faces - detects if faces are present. If present, generate coordinates, gender and age. ImageType - detects if image is clipart or a line drawing. Color - determines the accent color, dominant color, and whether an image is black&white.Adult - detects if the image is pornographic in nature (depicts nudity or a sex act).  Sexually suggestive content is also detected.
 * body parameter: details A string indicating which domain-specific details to return. Multiple values should be comma-separated. Valid visual feature types include:Celebrities - identifies celebrities if detected in the image.
 * body parameter: language The desired language for output generation. If this parameter is not specified, the default value is &quot;en&quot;.Supported languages:en - English, Default. es - Spanish, ja - Japanese, pt - Portuguese, zh - Simplified Chinese. Possible values include: 'en', 'es', 'ja', 'pt', 'zh'
 */
- (void) test__analyzeImage {
    XCTestExpectation *waitingLoading = [self expectationWithDescription:@"Wait for HTTP request to complete"];
    id<Protocol> op =[self.service];
    NSString* url = [NSString new]
    NSArray<VisualFeatureTypes*>* visualFeatures = [NSArray<VisualFeatureTypes*> new]
    NSArray<Details*>* details = [NSArray<Details*> new]
    NSString* language = [NSString new];
    [op analyzeImageWithUrl:url withVisualFeatures:visualFeatures withDetails:details withLanguage:language withCallback:^(ImageAnalysis* result, OperationError* error) {
        [waitingLoading fulfill];
        //XCTAssertNil(error, @"%@", error.reason);
    }];
    [self waitForExpectationsWithTimeout:100 handler:^(NSError *error) {
        if (error) {XCTFail(@"After block was not called.");}
    }];
}

/**
 * This operation generates a thumbnail image with the user-specified width and height. By default, the service analyzes the image, identifies the region of interest (ROI), and generates smart cropping coordinates based on the ROI. Smart cropping helps when you specify an aspect ratio that differs from that of the input image. A successful response contains the thumbnail image binary. If the request failed, the response contains an error code and a message to help determine what went wrong.
 *
 * body parameter: width Width of the thumbnail. It must be between 1 and 1024. Recommended minimum of 50.
 * body parameter: height Height of the thumbnail. It must be between 1 and 1024. Recommended minimum of 50.
 * body parameter: url Publicly reachable URL of an image
 * body parameter: smartCropping Boolean flag for enabling smart cropping.
 */
- (void) test__generateThumbnail {
    XCTestExpectation *waitingLoading = [self expectationWithDescription:@"Wait for HTTP request to complete"];
    id<Protocol> op =[self.service ];
    AZInteger* width = [AZInteger new]
    AZInteger* height = [AZInteger new]
    NSString* url = [NSString new]
    AZBoolean* smartCropping = [AZBoolean new];
    [op generateThumbnailWithWidth:width withHeight:height withUrl:url withSmartCropping:smartCropping withCallback:^(AZStream* result, OperationError* error) {
        [waitingLoading fulfill];
        //XCTAssertNil(error, @"%@", error.reason);
    }];
    [self waitForExpectationsWithTimeout:100 handler:^(NSError *error) {
        if (error) {XCTFail(@"After block was not called.");}
    }];
}

/**
 * Optical Character Recognition (OCR) detects printed text in an image and extracts the recognized characters into a machine-usable character stream.   Upon success, the OCR results will be returned. Upon failure, the error code together with an error message will be returned. The error code can be one of InvalidImageUrl, InvalidImageFormat, InvalidImageSize, NotSupportedImage,  NotSupportedLanguage, or InternalServerError.
 *
 * body parameter: detectOrientation Whether detect the text orientation in the image. With detectOrientation=true the OCR service tries to detect the image orientation and correct it before further processing (e.g. if it's upside-down).
 * body parameter: url Publicly reachable URL of an image
 * body parameter: language The BCP-47 language code of the text to be detected in the image. The default value is 'unk'. Possible values include: 'unk', 'zh-Hans', 'zh-Hant', 'cs', 'da', 'nl', 'en', 'fi', 'fr', 'de', 'el', 'hu', 'it', 'ja', 'ko', 'nb', 'pl', 'pt', 'ru', 'es', 'sv', 'tr', 'ar', 'ro', 'sr-Cyrl', 'sr-Latn', 'sk'
 */
- (void) test__recognizePrintedText {
    XCTestExpectation *waitingLoading = [self expectationWithDescription:@"Wait for HTTP request to complete"];
    id<Protocol> op =[self.service ];
    AZBoolean* detectOrientation = [AZBoolean new]
    NSString* url = [NSString new]
    OcrLanguages* language = [OcrLanguages new];
    [op recognizePrintedTextWithDetectOrientation:detectOrientation withUrl:url withLanguage:language withCallback:^(OcrResult* result, OperationError* error) {
        [waitingLoading fulfill];
        //XCTAssertNil(error, @"%@", error.reason);
    }];
    [self waitForExpectationsWithTimeout:100 handler:^(NSError *error) {
        if (error) {XCTFail(@"After block was not called.");}
    }];
}

/**
 * This operation generates a description of an image in human readable language with complete sentences.  The description is based on a collection of content tags, which are also returned by the operation. More than one description can be generated for each image.  Descriptions are ordered by their confidence score. All descriptions are in English. Two input methods are supported -- (1) Uploading an image or (2) specifying an image URL.A successful response will be returned in JSON.  If the request failed, the response will contain an error code and a message to help understand what went wrong.
 *
 * body parameter: url Publicly reachable URL of an image
 * body parameter: maxCandidates Maximum number of candidate descriptions to be returned.  The default is 1.
 * body parameter: language The desired language for output generation. If this parameter is not specified, the default value is &quot;en&quot;.Supported languages:en - English, Default. es - Spanish, ja - Japanese, pt - Portuguese, zh - Simplified Chinese. Possible values include: 'en', 'es', 'ja', 'pt', 'zh'
 */
- (void) test__describeImage {
    XCTestExpectation *waitingLoading = [self expectationWithDescription:@"Wait for HTTP request to complete"];
    id<Protocol> op =[self.service ];
    NSString* url = [NSString new]
    AZInteger* maxCandidates = [AZInteger new]
    NSString* language = [NSString new];
    [op describeImageWithUrl:url withMaxCandidates:maxCandidates withLanguage:language withCallback:^(ImageDescription* result, OperationError* error) {
        [waitingLoading fulfill];
        //XCTAssertNil(error, @"%@", error.reason);
    }];
    [self waitForExpectationsWithTimeout:100 handler:^(NSError *error) {
        if (error) {XCTFail(@"After block was not called.");}
    }];
}

/**
 * This operation generates a list of words, or tags, that are relevant to the content of the supplied image. The Computer Vision API can return tags based on objects, living beings, scenery or actions found in images. Unlike categories, tags are not organized according to a hierarchical classification system, but correspond to image content. Tags may contain hints to avoid ambiguity or provide context, for example the tag 'cello' may be accompanied by the hint 'musical instrument'. All tags are in English.
 *
 * body parameter: url Publicly reachable URL of an image
 * body parameter: language The desired language for output generation. If this parameter is not specified, the default value is &quot;en&quot;.Supported languages:en - English, Default. es - Spanish, ja - Japanese, pt - Portuguese, zh - Simplified Chinese. Possible values include: 'en', 'es', 'ja', 'pt', 'zh'
 */
- (void) test__tagImage {
    XCTestExpectation *waitingLoading = [self expectationWithDescription:@"Wait for HTTP request to complete"];
    id<Protocol> op =[self.service ];
    NSString* url = [NSString new]
    NSString* language = [NSString new];
    [op tagImageWithUrl:url withLanguage:language withCallback:^(TagResult* result, OperationError* error) {
        [waitingLoading fulfill];
        //XCTAssertNil(error, @"%@", error.reason);
    }];
    [self waitForExpectationsWithTimeout:100 handler:^(NSError *error) {
        if (error) {XCTFail(@"After block was not called.");}
    }];
}

/**
 * This operation recognizes content within an image by applying a domain-specific model.  The list of domain-specific models that are supported by the Computer Vision API can be retrieved using the /models GET request.  Currently, the API only provides a single domain-specific model: celebrities. Two input methods are supported -- (1) Uploading an image or (2) specifying an image URL. A successful response will be returned in JSON.  If the request failed, the response will contain an error code and a message to help understand what went wrong.
 *
 * body parameter: model The domain-specific content to recognize.
 * body parameter: url Publicly reachable URL of an image
 * body parameter: language The desired language for output generation. If this parameter is not specified, the default value is &quot;en&quot;.Supported languages:en - English, Default. es - Spanish, ja - Japanese, pt - Portuguese, zh - Simplified Chinese. Possible values include: 'en', 'es', 'ja', 'pt', 'zh'
 */
- (void) test__analyzeImageByDomain {
    XCTestExpectation *waitingLoading = [self expectationWithDescription:@"Wait for HTTP request to complete"];
    id<Protocol> op =[self.service ];
    NSString* model = [NSString new]
    NSString* url = [NSString new]
    NSString* language = [NSString new];
    [op analyzeImageByDomainWithModel:model withUrl:url withLanguage:language withCallback:^(DomainModelResults* result, OperationError* error) {
        [waitingLoading fulfill];
        //XCTAssertNil(error, @"%@", error.reason);
    }];
    [self waitForExpectationsWithTimeout:100 handler:^(NSError *error) {
        if (error) {XCTFail(@"After block was not called.");}
    }];
}

/**
 * This operation extracts a rich set of visual features based on the image content.
 *
 * body parameter: image An image stream.
 * body parameter: visualFeatures A string indicating what visual feature types to return. Multiple values should be comma-separated. Valid visual feature types include:Categories - categorizes image content according to a taxonomy defined in documentation. Tags - tags the image with a detailed list of words related to the image content. Description - describes the image content with a complete English sentence. Faces - detects if faces are present. If present, generate coordinates, gender and age. ImageType - detects if image is clipart or a line drawing. Color - determines the accent color, dominant color, and whether an image is black&white.Adult - detects if the image is pornographic in nature (depicts nudity or a sex act).  Sexually suggestive content is also detected.
 * body parameter: details A string indicating which domain-specific details to return. Multiple values should be comma-separated. Valid visual feature types include:Celebrities - identifies celebrities if detected in the image.
 * body parameter: language The desired language for output generation. If this parameter is not specified, the default value is &quot;en&quot;.Supported languages:en - English, Default. es - Spanish, ja - Japanese, pt - Portuguese, zh - Simplified Chinese. Possible values include: 'en', 'es', 'ja', 'pt', 'zh'
 */
- (void) test__analyzeImageInStream {
    XCTestExpectation *waitingLoading = [self expectationWithDescription:@"Wait for HTTP request to complete"];
    id<Protocol> op =[self.service ];
    AZStream* image = [AZStream new]
    NSArray<VisualFeatureTypes*>* visualFeatures = [NSArray<VisualFeatureTypes*> new]
    NSArray<Details*>* details = [NSArray<Details*> new]
    NSString* language = [NSString new];
    [op analyzeImageInStreamWithImage:image withVisualFeatures:visualFeatures withDetails:details withLanguage:language withCallback:^(ImageAnalysis* result, OperationError* error) {
        [waitingLoading fulfill];
        //XCTAssertNil(error, @"%@", error.reason);
    }];
    [self waitForExpectationsWithTimeout:100 handler:^(NSError *error) {
        if (error) {XCTFail(@"After block was not called.");}
    }];
}

/**
 * This operation generates a thumbnail image with the user-specified width and height. By default, the service analyzes the image, identifies the region of interest (ROI), and generates smart cropping coordinates based on the ROI. Smart cropping helps when you specify an aspect ratio that differs from that of the input image. A successful response contains the thumbnail image binary. If the request failed, the response contains an error code and a message to help determine what went wrong.
 *
 * body parameter: width Width of the thumbnail. It must be between 1 and 1024. Recommended minimum of 50.
 * body parameter: height Height of the thumbnail. It must be between 1 and 1024. Recommended minimum of 50.
 * body parameter: image An image stream.
 * body parameter: smartCropping Boolean flag for enabling smart cropping.
 */
- (void) test__generateThumbnailInStream {
    XCTestExpectation *waitingLoading = [self expectationWithDescription:@"Wait for HTTP request to complete"];
    id<Protocol> op =[self.service ];
    AZInteger* width = [AZInteger new]
    AZInteger* height = [AZInteger new]
    AZStream* image = [AZStream new]
    AZBoolean* smartCropping = [AZBoolean new];
    [op generateThumbnailInStreamWithWidth:width withHeight:height withImage:image withSmartCropping:smartCropping withCallback:^(AZStream* result, OperationError* error) {
        [waitingLoading fulfill];
        //XCTAssertNil(error, @"%@", error.reason);
    }];
    [self waitForExpectationsWithTimeout:100 handler:^(NSError *error) {
        if (error) {XCTFail(@"After block was not called.");}
    }];
}

/**
 * Optical Character Recognition (OCR) detects printed text in an image and extracts the recognized characters into a machine-usable character stream.   Upon success, the OCR results will be returned. Upon failure, the error code together with an error message will be returned. The error code can be one of InvalidImageUrl, InvalidImageFormat, InvalidImageSize, NotSupportedImage,  NotSupportedLanguage, or InternalServerError.
 *
 * body parameter: detectOrientation Whether detect the text orientation in the image. With detectOrientation=true the OCR service tries to detect the image orientation and correct it before further processing (e.g. if it's upside-down).
 * body parameter: image An image stream.
 * body parameter: language The BCP-47 language code of the text to be detected in the image. The default value is 'unk'. Possible values include: 'unk', 'zh-Hans', 'zh-Hant', 'cs', 'da', 'nl', 'en', 'fi', 'fr', 'de', 'el', 'hu', 'it', 'ja', 'ko', 'nb', 'pl', 'pt', 'ru', 'es', 'sv', 'tr', 'ar', 'ro', 'sr-Cyrl', 'sr-Latn', 'sk'
 */
- (void) test__recognizePrintedTextInStream {
    XCTestExpectation *waitingLoading = [self expectationWithDescription:@"Wait for HTTP request to complete"];
    id<Protocol> op =[self.service ];
    AZBoolean* detectOrientation = [AZBoolean new]
    AZStream* image = [AZStream new]
    OcrLanguages* language = [OcrLanguages new];
    [op recognizePrintedTextInStreamWithDetectOrientation:detectOrientation withImage:image withLanguage:language withCallback:^(OcrResult* result, OperationError* error) {
        [waitingLoading fulfill];
        //XCTAssertNil(error, @"%@", error.reason);
    }];
    [self waitForExpectationsWithTimeout:100 handler:^(NSError *error) {
        if (error) {XCTFail(@"After block was not called.");}
    }];
}

/**
 * This operation generates a description of an image in human readable language with complete sentences.  The description is based on a collection of content tags, which are also returned by the operation. More than one description can be generated for each image.  Descriptions are ordered by their confidence score. All descriptions are in English. Two input methods are supported -- (1) Uploading an image or (2) specifying an image URL.A successful response will be returned in JSON.  If the request failed, the response will contain an error code and a message to help understand what went wrong.
 *
 * body parameter: image An image stream.
 * body parameter: maxCandidates Maximum number of candidate descriptions to be returned.  The default is 1.
 * body parameter: language The desired language for output generation. If this parameter is not specified, the default value is &quot;en&quot;.Supported languages:en - English, Default. es - Spanish, ja - Japanese, pt - Portuguese, zh - Simplified Chinese. Possible values include: 'en', 'es', 'ja', 'pt', 'zh'
 */
- (void) test__describeImageInStream {
    XCTestExpectation *waitingLoading = [self expectationWithDescription:@"Wait for HTTP request to complete"];
    id<Protocol> op =[self.service ];
    AZStream* image = [AZStream new]
    AZInteger* maxCandidates = [AZInteger new]
    NSString* language = [NSString new];
    [op describeImageInStreamWithImage:image withMaxCandidates:maxCandidates withLanguage:language withCallback:^(ImageDescription* result, OperationError* error) {
        [waitingLoading fulfill];
        //XCTAssertNil(error, @"%@", error.reason);
    }];
    [self waitForExpectationsWithTimeout:100 handler:^(NSError *error) {
        if (error) {XCTFail(@"After block was not called.");}
    }];
}

/**
 * This operation generates a list of words, or tags, that are relevant to the content of the supplied image. The Computer Vision API can return tags based on objects, living beings, scenery or actions found in images. Unlike categories, tags are not organized according to a hierarchical classification system, but correspond to image content. Tags may contain hints to avoid ambiguity or provide context, for example the tag 'cello' may be accompanied by the hint 'musical instrument'. All tags are in English.
 *
 * body parameter: image An image stream.
 * body parameter: language The desired language for output generation. If this parameter is not specified, the default value is &quot;en&quot;.Supported languages:en - English, Default. es - Spanish, ja - Japanese, pt - Portuguese, zh - Simplified Chinese. Possible values include: 'en', 'es', 'ja', 'pt', 'zh'
 */
- (void) test__tagImageInStream {
    XCTestExpectation *waitingLoading = [self expectationWithDescription:@"Wait for HTTP request to complete"];
    id<Protocol> op =[self.service ];
    AZStream* image = [AZStream new]
    NSString* language = [NSString new];
    [op tagImageInStreamWithImage:image withLanguage:language withCallback:^(TagResult* result, OperationError* error) {
        [waitingLoading fulfill];
        //XCTAssertNil(error, @"%@", error.reason);
    }];
    [self waitForExpectationsWithTimeout:100 handler:^(NSError *error) {
        if (error) {XCTFail(@"After block was not called.");}
    }];
}

/**
 * This operation recognizes content within an image by applying a domain-specific model.  The list of domain-specific models that are supported by the Computer Vision API can be retrieved using the /models GET request.  Currently, the API only provides a single domain-specific model: celebrities. Two input methods are supported -- (1) Uploading an image or (2) specifying an image URL. A successful response will be returned in JSON.  If the request failed, the response will contain an error code and a message to help understand what went wrong.
 *
 * body parameter: model The domain-specific content to recognize.
 * body parameter: image An image stream.
 * body parameter: language The desired language for output generation. If this parameter is not specified, the default value is &quot;en&quot;.Supported languages:en - English, Default. es - Spanish, ja - Japanese, pt - Portuguese, zh - Simplified Chinese. Possible values include: 'en', 'es', 'ja', 'pt', 'zh'
 */
- (void) test__analyzeImageByDomainInStream {
    XCTestExpectation *waitingLoading = [self expectationWithDescription:@"Wait for HTTP request to complete"];
    id<Protocol> op =[self.service ];
    NSString* model = [NSString new]
    AZStream* image = [AZStream new]
    NSString* language = [NSString new];
    [op analyzeImageByDomainInStreamWithModel:model withImage:image withLanguage:language withCallback:^(DomainModelResults* result, OperationError* error) {
        [waitingLoading fulfill];
        //XCTAssertNil(error, @"%@", error.reason);
    }];
    [self waitForExpectationsWithTimeout:100 handler:^(NSError *error) {
        if (error) {XCTFail(@"After block was not called.");}
    }];
}


//- (void)testPerformanceExample {
//    // This is an example of a performance test case.
//    [self measureBlock:^{
//        // Put the code you want to measure the time of here.
//    }];
//}
@end
